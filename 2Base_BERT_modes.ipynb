{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2Base BERT modes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thingumajig/colab-experiments/blob/master/2Base_BERT_modes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFxy9VfywWj0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1476
        },
        "outputId": "19645395-93a7-4760-d607-d775d96cbf56"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# https://github.com/google-research/bert\n",
        "# https://github.com/CyberZHG/keras-bert\n",
        "\n",
        "# папка, куда распаковать преодобученную нейросеть BERT\n",
        "folder = 'multi_cased_L-12_H-768_A-12'\n",
        "download_url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip'  # ссылка на скачивание модели\n",
        "\n",
        "print('Downloading model...')\n",
        "zip_path = '{}.zip'.format(folder)\n",
        "!test -d $folder || (wget $download_url && unzip $zip_path)\n",
        "\n",
        "# скачиваем из BERT репозитория файл tokenization.py\n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
        "\n",
        "# install Keras BERT\n",
        "!pip install keras-bert\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "import tokenization\n",
        "\n",
        "config_path = folder+'/bert_config.json'\n",
        "checkpoint_path = folder+'/bert_model.ckpt'\n",
        "vocab_path = folder+'/vocab.txt'\n",
        "\n",
        "# создаем объект для перевода строки с пробелами в токены\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n",
        "\n",
        "# загружаем модель\n",
        "print('Loading model...')\n",
        "model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n",
        "#model.summary()          # информация о слоях нейросети - количество параметров и т.д.\n",
        "print('OK')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading model...\n",
            "--2019-05-30 16:56:13--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M   167MB/s    in 3.8s    \n",
            "\n",
            "2019-05-30 16:56:17 (167 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n",
            "Archive:  multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n",
            "--2019-05-30 16:56:26--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12257 (12K) [text/plain]\n",
            "Saving to: ‘tokenization.py’\n",
            "\n",
            "tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-05-30 16:56:27 (172 MB/s) - ‘tokenization.py’ saved [12257/12257]\n",
            "\n",
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/f1/02896ce76c132761ba21fed5ea9461ffa7c69e7a9beb76ff7657d69f98f3/keras-bert-0.55.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.16.3)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.2.4)\n",
            "Collecting keras-pos-embd==0.10.0 (from keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/01/e4/8b8519779d84c412c2c1bbf4637066800c3b04463da059a5d220fa904133/keras-pos-embd-0.10.0.tar.gz\n",
            "Collecting keras-transformer==0.23.0 (from keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/31/0d/0b62504dc9a6377d035b651b9222c95f9094c002bdcb17162ad4863de6ba/keras-transformer-0.23.0.tar.gz\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.3.0)\n",
            "Collecting keras-multi-head==0.20.0 (from keras-transformer==0.23.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/0f/f1a66974db9c328ba675c1df63f8a68c5c0f3e181f1e74db4f3b1a72a6df/keras-multi-head-0.20.0.tar.gz\n",
            "Collecting keras-layer-normalization==0.12.0 (from keras-transformer==0.23.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/76/42878fe46bff8458d8aa1da50bfdf705d632d33dbb7b60db52a06faf2dad/keras-layer-normalization-0.12.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward==0.5.0 (from keras-transformer==0.23.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/51/a8/b9dec35117d0fb16d07bd8c216540c4ab42e93dd3d9ca43345cb01bbdc3f/keras-position-wise-feed-forward-0.5.0.tar.gz\n",
            "Collecting keras-embed-sim==0.4.0 (from keras-transformer==0.23.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/80/c7573cdb2344bc93824c9327f390e40dabe32dc895087651090baa134999/keras-embed-sim-0.4.0.tar.gz\n",
            "Collecting keras-self-attention==0.41.0 (from keras-multi-head==0.20.0->keras-transformer==0.23.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-pos-embd, keras-transformer, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/2e/bf/7b92973a0164059cee3deeb718295911baef8bb62dcc344ca2\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/bc/07/35f14835eaaca28a99d33b1c6f36df6aeae052161abb8af465\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/89/ee/620c1c8de642ab21ac8236b0dc77cab7a0dce2f59fc88cd468\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/49/02/4eda210bc4c37ff1d45311665bceb790881dbea92b27b025a5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/9b/9e/f4072915f660e90bb3638332276f4de80476f3afcb5d010d6f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/58/cc/007f963a01c8304c793f9e887d50ff6e7c495343287c01a6a2\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/71/f6/2c51739d83b0c874a491698ed345293dfa62c7ce52798b86bd\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-bert keras-pos-embd keras-transformer keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.55.1 keras-embed-sim-0.4.0 keras-layer-normalization-0.12.0 keras-multi-head-0.20.0 keras-pos-embd-0.10.0 keras-position-wise-feed-forward-0.5.0 keras-self-attention-0.41.0 keras-transformer-0.23.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zikPBVg8wkjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "74b9782f-2bd0-4b5a-d30e-34515fb7be71"
      },
      "source": [
        "\n",
        "# РЕЖИМ 1: предсказание слов, закрытых токеном MASK в фразе. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в [MASK] и купил [MASK]. [SEP]\n",
        "\n",
        "# входная фраза с закрытыми словами с помощью [MASK]\n",
        "sentence = '\\u042F \\u043F\\u0440\\u0438\\u0448\\u0435\\u043B \\u0432 [MASK] \\u0438 \\u043A\\u0443\\u043F\\u0438\\u043B [MASK].'  #@param {type:\"string\"}\n",
        "\n",
        "print(sentence)\n",
        "\n",
        "\n",
        "#-------------------------\n",
        "# преобразование в токены (tokenizer.tokenize() не обрабатывает [CLS], [MASK], поэтому добавим их вручную)\n",
        "sentence = sentence.replace(' [MASK] ','[MASK]'); sentence = sentence.replace('[MASK] ','[MASK]'); sentence = sentence.replace(' [MASK]','[MASK]')  # удаляем лишние пробелы\n",
        "sentence = sentence.split('[MASK]')             # разбиваем строку по маске\n",
        "tokens = ['[CLS]']                              # фраза всегда должна начинаться на [CLS]\n",
        "# обычные строки преобразуем в токены с помощью tokenizer.tokenize(), вставляя между ними [MASK]\n",
        "for i in range(len(sentence)):\n",
        "    if i == 0:\n",
        "        tokens = tokens + tokenizer.tokenize(sentence[i]) \n",
        "    else:\n",
        "        tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) \n",
        "tokens = tokens + ['[SEP]']                     # фраза всегда должна заканчиваться на [SEP] \n",
        "# в tokens теперь токены, которые гарантированно по словарю преобразуются в индексы\n",
        "#-------------------------\n",
        "#print(tokens)\n",
        "\n",
        "# преобразуем в массив индексов, который можно подавать на вход сети, причем число 103 в нем это [MASK]\n",
        "token_input = tokenizer.convert_tokens_to_ids(tokens)        \n",
        "#print(token_input)\n",
        "# удлиняем до 512 длины\n",
        "token_input = token_input + [0] * (512 - len(token_input))\n",
        "\n",
        "\n",
        "# создаем маску, заменив все числа 103 на 1, а остальное 0\n",
        "mask_input = [0]*512\n",
        "for i in range(len(mask_input)):\n",
        "    if token_input[i] == 103:\n",
        "        mask_input[i] = 1\n",
        "#print(mask_input)\n",
        "\n",
        "# маска фраз (вторая фраза маскируется числом 1, а все остальное числом 0)\n",
        "seg_input = [0]*512\n",
        "\n",
        "\n",
        "# конвертируем в numpy в форму (1,) -> (1,512)\n",
        "token_input = np.asarray([token_input])\n",
        "mask_input = np.asarray([mask_input])\n",
        "seg_input = np.asarray([seg_input])\n",
        "\n",
        "\n",
        "# пропускаем через нейросеть...\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[0]       # в [0] полная фраза с заполненными предсказанными словами на месте [MASK]\n",
        "predicts = np.argmax(predicts, axis=-1)\n",
        "\n",
        "\n",
        "# форматируем результат в строку, разделенную пробелами\n",
        "predicts = predicts[0][:len(tokens)]    # длиной как исходная фраза (чтобы отсечь случайные выбросы среди нулей дальше)\n",
        "out = []\n",
        "# добавляем в out только слова в позиции [MASK], которые маскированы цифрой 1 в mask_input\n",
        "for i in range(len(mask_input[0])):\n",
        "    if mask_input[0][i] == 1:           # [0][i], т.к. требование было (1,512)\n",
        "        out.append(predicts[i]) \n",
        "\n",
        "out = tokenizer.convert_ids_to_tokens(out)      # индексы в токены\n",
        "out = ' '.join(out)                             # объединяем в одну строку с пробелами\n",
        "out = tokenization.printable_text(out)          # в читабельную версию\n",
        "out = out.replace(' ##','')                     # объединяем раздъединенные слова \"при ##шел\" -> \"пришел\"\n",
        "print('Result:', out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Я пришел в [MASK] и купил [MASK].\n",
            "Result: дом его\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjXLJupvwoFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "db7f2461-6828-44ff-e250-1dbc8fd03849"
      },
      "source": [
        "\n",
        "# РЕЖИМ 2: проверка логичности двух фраз. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в магазин. [SEP] И купил молоко. [SEP]\n",
        "\n",
        "sentence_1 = 'Consideration during the planning phase should include:  Drainage of control cable trenches; Design of control cable trenches; Design of safety screens. Compile a checklist of items that should be addressed and considered during the planning and construction phase.  Use the checklist to ensure that there is enough budget, planning, tools, resources, support for the required implementation/project.'      #@param {type:\"string\"}\n",
        "sentence_2 = 'Planning and construction phases should be completed in phases. '          #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "print(sentence_1, '->', sentence_2)\n",
        "\n",
        "# строки в массивы токенов\n",
        "tokens_sen_1 = tokenizer.tokenize(sentence_1)\n",
        "tokens_sen_2 = tokenizer.tokenize(sentence_2)\n",
        "\n",
        "tokens = ['[CLS]'] + tokens_sen_1 + ['[SEP]'] + tokens_sen_2 + ['[SEP]']\n",
        "#print(tokens)\n",
        "\n",
        "# преобразуем строковые токены в числовые индексы:\n",
        "token_input = tokenizer.convert_tokens_to_ids(tokens)  \n",
        "# удлиняем до 512      \n",
        "token_input = token_input + [0] * (512 - len(token_input))\n",
        "\n",
        "# маска в этом режиме все 0\n",
        "mask_input = [0] * 512\n",
        "\n",
        "# в маске предложений под второй фразой, включая конечный SEP, надо поставить 1, а все остальное заполнить 0\n",
        "seg_input = [0]*512\n",
        "len_1 = len(tokens_sen_1) + 2                   # длина первой фразы, +2 - включая начальный CLS и разделитель SEP\n",
        "for i in range(len(tokens_sen_2)+1):            # +1, т.к. включая последний SEP\n",
        "        seg_input[len_1 + i] = 1                # маскируем вторую фразу, включая последний SEP, единицами\n",
        "#print(seg_input)\n",
        "\n",
        "\n",
        "# конвертируем в numpy в форму (1,) -> (1,512)\n",
        "token_input = np.asarray([token_input])\n",
        "mask_input = np.asarray([mask_input])\n",
        "seg_input = np.asarray([seg_input])\n",
        "\n",
        "\n",
        "# пропускаем через нейросеть...\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[1]       # в [1] ответ на вопрос, является ли второе предложение логичным по смыслу\n",
        "#print('Sentence is okey: ', not bool(np.argmax(predicts, axis=-1)[0]), predicts)\n",
        "print('Sentence is okey:', int(round(predicts[0][0]*100)), '%')                    # [[0.9657724  0.03422766]] - левое число вероятность что второе предложение подходит по смыслу, а правое - что второе предложение случайное\n",
        "out = int(round(predicts[0][0]*100)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Consideration during the planning phase should include:  Drainage of control cable trenches; Design of control cable trenches; Design of safety screens. Compile a checklist of items that should be addressed and considered during the planning and construction phase.  Use the checklist to ensure that there is enough budget, planning, tools, resources, support for the required implementation/project. -> Planning and construction phases should be completed in phases. \n",
            "Sentence is okey: 100 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}